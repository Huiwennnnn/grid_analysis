{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T20:59:29.370038Z",
     "start_time": "2024-04-16T20:59:29.359596Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyomo\n",
    "import scipy\n",
    "import pyomo.environ as pyo\n",
    "from pyomo.environ import *\n",
    "from pyomo.util.infeasible import log_infeasible_constraints\n",
    "from datetime import datetime,timedelta\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.cluster import KMeans\n",
    "import logging\n",
    "import os\n",
    "pd.set_option('display.max_columns', None)  # or specify a number if you want a limit\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62b80a127ee3e3a6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic Setting\n",
    "grid = \"369_0\"\n",
    "next_two = True # True consider next 2 trips,False consider only next 1 trip\n",
    "scenario_year = 2050\n",
    "weekday_dict = {'Monday':(7,4), 'Tuesday':(7,5),'Wednesday':(7,6),'Thursday':(7,7),'Friday':(7,8),'Saturday':(7,9),'Sunday':(7,10)}\n",
    "# weekday_dict = {'Monday':(1,3), 'Tuesday':(1,4),'Wednesday':(1,5),'Thursday':(1,6),'Friday':(1,7),'Saturday':(1,8),'Sunday':(1,9)}\n",
    "# weekday_dict = {'Monday':(1,10), 'Tuesday':(1,11),'Wednesday':(1,12),'Thursday':(1,13),'Friday':(1,14),'Saturday':(1,15),'Sunday':(1,16)}\n",
    "day_start_ts = pd.to_datetime(f\"{scenario_year}-07-08 00:00:00\")\n",
    "day_start = day_start_ts.day\n",
    "day_end_ts = pd.to_datetime(f\"{scenario_year}-07-09 00:00:00\")\n",
    "monitor_hr = int((day_end_ts - day_start_ts).total_seconds()/3600)\n",
    "weekday = \"Friday\"\n",
    "path = f\"{grid}/{scenario_year}_{weekday}_07_08_controlled\"\n",
    "sanitycheck_path = f\"{path}/sanitycheck\"\n",
    "\n",
    "participate_rate = 1  # 1 no constraints on participation rate, parking event based, not vehicle based\n",
    "min_power_level = 0.5 # 1 constant power request at max. charger power, otherwise set a lower limit on power requested: min_power_level * max_power\n",
    "# Calculate the number of groups\n",
    "num_groups = 30\n",
    "TP_dict = {1:2,2:2,3:2,4:3,5:3,6:4,7:4,8:4,9:4,10:3,11:2,12:1}\n",
    "TP = TP_dict[day_start_ts.month] # 1:Dec, 2:Jan,Feb,Mar,Nov, 3:Apr,May,Oct, 4:Jun,Jul,Aug,Sep\n",
    "os.makedirs(path,exist_ok=True)\n",
    "os.makedirs(sanitycheck_path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5174c4ee694415d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T21:00:36.834388Z",
     "start_time": "2024-04-16T21:00:36.749591Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "def calculate_next_trip_e(row):\n",
    "    if next_two and (row['next_SoE_bc']+(row['next_parking_time']//60*7) < row[f'next_2_travel_TP{TP}_consumption']): # if charge with 7 KW during whole next pakring event could not cover energy requirement for the second next trip charge the part that could not be covered within this parking event\n",
    "        next_trip_e = row[f'next_travel_TP{TP}_consumption']+ row[f'next_2_travel_TP{TP}_consumption'] - row['next_SoE_bc']+(row['next_parking_time']//60*7)#row['next_2_travel_TP1_consumption']\n",
    "    else:\n",
    "        next_trip_e =  row[f'next_travel_TP{TP}_consumption']\n",
    "    return next_trip_e\n",
    "\n",
    "def get_hour_index(ts):\n",
    "    idx = int(divmod((ts-day_start_ts).total_seconds(),3600)[0]) \n",
    "    return idx\n",
    "\n",
    "def get_soe_init(SoE_bc,arr_idx):\n",
    "    if arr_idx<=0:\n",
    "        arr_idx = 1\n",
    "    soe_init_list = [0]*monitor_hr\n",
    "    for idx in range(arr_idx):\n",
    "        soe_init_list[idx] = SoE_bc\n",
    "    return soe_init_list\n",
    "\n",
    "\n",
    "def create_dict(start_ts,end_ts,start_hour_idx,end_hour_idx):\n",
    "    # if end_hour_idx>monitor_hr-1:\n",
    "    #     end_hour_idx = monitor_hr-1\n",
    "    dict = [0]*monitor_hr\n",
    "    for hour in range(monitor_hr):\n",
    "        if start_hour_idx<hour and end_hour_idx>hour:\n",
    "            dict[hour] = 60\n",
    "        elif start_hour_idx==hour and end_hour_idx==hour:\n",
    "            dict[hour] = end_ts.minute-start_ts.minute\n",
    "        elif start_hour_idx<hour and end_hour_idx==hour:\n",
    "            dict[hour] = end_ts.minute\n",
    "        elif start_hour_idx==hour and end_hour_idx>hour:\n",
    "            dict[hour] = 60-start_ts.minute\n",
    "    return dict\n",
    "\n",
    "def create_charge_time_list(charge_decision, start_ts, end_ts, start_hour_idx,end_hour_idx):\n",
    "    # if end_hour_idx>monitor_hr-1:\n",
    "    #     end_hour_idx = monitor_hr-1\n",
    "    dict = [0]*monitor_hr\n",
    "    if charge_decision:\n",
    "        for hour in range(monitor_hr):\n",
    "            if start_hour_idx<hour and end_hour_idx>hour:\n",
    "                dict[hour] = 60\n",
    "            elif start_hour_idx==hour and end_hour_idx==hour:\n",
    "                dict[hour] = end_ts.minute-start_ts.minute\n",
    "            elif start_hour_idx<hour and end_hour_idx==hour:\n",
    "                dict[hour] = end_ts.minute\n",
    "            elif start_hour_idx==hour and end_hour_idx>hour:\n",
    "                dict[hour] = 60-start_ts.minute\n",
    "    return dict\n",
    "\n",
    "\n",
    "# Optimization\n",
    "def create_sparse_dict(value_list,monitior_hr):\n",
    "    value_indices = value_list.index.to_numpy()\n",
    "    values = np.array(value_list.tolist())\n",
    "    row_indices = np.repeat(value_indices,monitor_hr)\n",
    "    col_indices = np.tile(np.arange(monitior_hr),len(value_indices))\n",
    "    data_values = values.flatten()\n",
    "\n",
    "    # Filter out zero values to keep the matrix sparse\n",
    "    non_zero_mask = data_values != 0\n",
    "    row_indices = row_indices[non_zero_mask].astype(int)\n",
    "    col_indices = col_indices[non_zero_mask].astype(int)\n",
    "    data_values = data_values[non_zero_mask]\n",
    "    \n",
    "    sparse_matrix = scipy.sparse.coo_matrix((data_values,(row_indices,col_indices)), shape=(max(value_indices)+1,monitor_hr))\n",
    "    return sparse_matrix\n",
    "\n",
    "    \n",
    "# Data Postprocessing\n",
    "def get_timestamp_pair(row):\n",
    "    process = {}\n",
    "    process_key = ()\n",
    "    power = []\n",
    "    for hour in range(monitor_hr):\n",
    "        p_t = row['optimized_power_list'][hour]\n",
    "        min_t = row['hourly_time_dict'][hour]\n",
    "        if hour>0:\n",
    "            min_pre, p_pre = row['hourly_time_dict'][hour-1],row['optimized_power_list'][hour-1]\n",
    "        else: \n",
    "            min_pre,p_pre = 0,0\n",
    "        if hour<(monitor_hr-1):\n",
    "            min_next, p_next = row['hourly_time_dict'][hour+1],row['optimized_power_list'][hour+1]\n",
    "        else:\n",
    "            min_next, p_next = 0,0\n",
    "        if p_pre==0 and p_t!=0:\n",
    "            if (min_t<=60) and (min_pre>0):\n",
    "                start_min = 0\n",
    "            elif (min_pre==0):\n",
    "                start_min = 60-min_t\n",
    "            # elif (min_t == 60 or min_pre==60) and (hour!=row['arr_time_idx']):\n",
    "            #     start_min = 0\n",
    "            # elif (min_t<60 and min_t>0):# and (hour!=row['arr_time_idx']): #and min_pre!=60 \n",
    "            #     start_min = 60-min_t\n",
    "            # elif (hour==row['arr_time_idx']):\n",
    "            #     start_min = row['arr_time'].minute\n",
    "\n",
    "            else:\n",
    "                start_min = 0\n",
    "\n",
    "            start_delta_hr = day_start_ts + timedelta(hours=hour)\n",
    "            start_ts = pd.Timestamp(datetime(year=start_delta_hr.year,month=start_delta_hr.month,day=start_delta_hr.day,hour=start_delta_hr.hour,minute=start_min))\n",
    "            process_key = (start_ts,)\n",
    "        if p_t!=0:\n",
    "            power.append(p_t)\n",
    "        if p_next==0 and p_t!=0:\n",
    "            if (min_t==60):\n",
    "                end_min=59\n",
    "            elif p_pre!=0 and min_t<60:\n",
    "                end_min = min_t\n",
    "            elif p_pre==0 and min_t<60:\n",
    "                end_min = start_min+min_t-1\n",
    "            # if (min_t==60 or min_next>0) and (hour!=row['park_end_time_idx']):\n",
    "            #     end_min=59\n",
    "            # elif (min_t<60 and min_next==0 and min_pre>0) and (hour!=row['park_end_time_idx']):\n",
    "            #     end_min=min_t\n",
    "            # elif (hour==row['park_end_time_idx']):\n",
    "            #     end_min=row['park_end_time'].minute\n",
    "            # else:\n",
    "            #     end_min = 0\n",
    "\n",
    "            end_delta_hr = day_start_ts + timedelta(hours=hour)\n",
    "            end_ts = pd.Timestamp(datetime(year=end_delta_hr.year,month=end_delta_hr.month,day=end_delta_hr.day,hour=end_delta_hr.hour,minute=end_min))\n",
    "\n",
    "            process_key = process_key + (end_ts,)\n",
    "            process[process_key] = power\n",
    "            process_key = ()\n",
    "            power = []\n",
    "    return process\n",
    "\n",
    "def check_outside(row):\n",
    "    if (row['process_cnt']==0) and (row['st_chg_time']==row['ed_chg_time']):\n",
    "        return False\n",
    "    elif row['process_cnt']>=1 and (((next(iter(row['process_list'].keys())))[0]>=row['st_chg_time']) and ((next(iter(row['process_list'].keys())))[1]<=row['ed_chg_time'])):\n",
    "        return False\n",
    "    elif row['process_cnt']>=1 and (((next(iter(row['process_list'].keys())))[0]>=row['ed_chg_time']) or ((next(iter(row['process_list'].keys())))[1]<=row['st_chg_time'])):\n",
    "        return True\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def correct_st_chg_time(row):\n",
    "    if row['process_list'] and row['optimized_process_mean_power']>0:\n",
    "        if row['c']==True and row['st_chg_time']<day_start_ts and row['ed_chg_time']>day_start_ts and next(iter(row['process_list'].keys()))[0]==day_start_ts:\n",
    "            return row['st_chg_time']\n",
    "        elif next(iter(row['process_list'].keys()))[0]>=day_start_ts:\n",
    "            return next(iter(row['process_list'].keys()))[0]\n",
    "    elif row['process_list'] and row['optimized_process_mean_power']<0:\n",
    "        return next(iter(row['process_list'].keys()))[0]\n",
    "    elif (row['c']==True and row['st_chg_time']<day_start_ts and row['ed_chg_time']<day_start_ts) or (row['c']==True and row['st_chg_time']>=day_end_ts and row['ed_chg_time']>=day_end_ts):\n",
    "        return row['st_chg_time']\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def correct_ed_chg_time(row):\n",
    "    if row['process_list'] and row['optimized_process_mean_power']>0:\n",
    "        if row['c']==True and row['ed_chg_time']>=day_end_ts-timedelta(minutes=1) and row['st_chg_time']<day_end_ts-timedelta(minutes=1) and next(iter(row['process_list'].keys()))[1]>=day_end_ts-timedelta(minutes=1) and row['day_end_soe']<row['B']:\n",
    "            return row['ed_chg_time']\n",
    "        elif next(iter(row['process_list'].keys()))[1]<=day_end_ts-timedelta(minutes=1):\n",
    "            return next(iter(row['process_list'].keys()))[1]\n",
    "    elif row['process_list'] and row['optimized_process_mean_power']<0:\n",
    "        return next(iter(row['process_list'].keys()))[1]\n",
    "    elif (row['c']==True and row['st_chg_time']<day_start_ts and row['ed_chg_time']<day_start_ts) or (row['c']==True and row['st_chg_time']>=day_end_ts and row['ed_chg_time']>=day_end_ts):\n",
    "        return row['ed_chg_time']\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bbb72876a34fe9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T21:03:40.887540Z",
     "start_time": "2024-04-16T21:01:39.390402Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/huiwen/Library/Mobile Documents/com~apple~CloudDocs/Thesis/mobility/grid369_mobility_dataset.csv\")\n",
    "for col in ['dep_time','arr_time','st_chg_time','ed_chg_time']:\n",
    "    df[col] = pd.to_datetime(df[col],format='mixed')\n",
    "df['chg_time'] = pd.to_timedelta(df['ed_chg_time']-df['st_chg_time'],unit='m')\n",
    "\n",
    "df['dep_time'] = df.apply(lambda row:row['dep_time'].replace(year=scenario_year,month=weekday_dict[row['type_day']][0],day=weekday_dict[row['type_day']][1]), axis=1)\n",
    "df['dep_time_idx'] = df.apply(lambda x:get_hour_index(x['dep_time']),axis=1)\n",
    "\n",
    "df['arr_time'] = df['dep_time'] + pd.to_timedelta(df['trav_time'], unit='m')\n",
    "df['arr_hour'] = df['arr_time'].dt.hour\n",
    "df['arr_time_idx'] = df.apply(lambda x:get_hour_index(x['arr_time']), axis=1)\n",
    "df['arr_day'] = df['arr_time'].dt.day\n",
    "\n",
    "df['park_end_time'] = df['arr_time']+pd.to_timedelta(df['parking_time'],unit='m')\n",
    "df['park_end_hour'] = df['park_end_time'].dt.hour\n",
    "df['park_end_time_idx'] = df.apply(lambda x:get_hour_index(x['park_end_time']), axis=1)\n",
    "df['park_end_day'] = df['park_end_time'].dt.day\n",
    "\n",
    "df['st_chg_time']=df['arr_time']\n",
    "df['st_chg_time_idx'] = df.apply(lambda x:get_hour_index(x['st_chg_time']),axis=1)\n",
    "df['ed_chg_time'] = df['st_chg_time']+df['chg_time']\n",
    "df['ed_chg_time_idx'] = df.apply(lambda x:get_hour_index(x['ed_chg_time']),axis=1)\n",
    "df['chg_time'] = df['chg_time'].dt.total_seconds()/60\n",
    "\n",
    "df.sort_values(by=['person','dep_time'])\n",
    "df_byperson = df.groupby('person')\n",
    "df[f'next_travel_TP{TP}_consumption'] = df_byperson[f'TP{TP} consumption kWh'].shift(-1).fillna(0)\n",
    "df['next_parking_time'] = df_byperson['parking_time'].shift(-1).fillna(0)\n",
    "df[f'next_2_travel_TP{TP}_consumption'] = df_byperson[f'TP{TP} consumption kWh'].shift(-2).fillna(0)\n",
    "df['next_SoE_bc'] = df_byperson['SoE_bc'].shift(-1).fillna(0)\n",
    "df['next_trip_e'] = df.apply(calculate_next_trip_e,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df.drop(['next_parking_time', 'st_mun_name', 'st_canton_name', 'st_urb_type',\n",
    "         'st_mnt_type', 'ed_mun_name', 'ed_canton_name', 'ed_urb_type',\n",
    "         'ed_mnt_type', 'Yearly kWh'], axis=1, inplace=True) \n",
    "# 'next_travel_TP1_consumption','next_2_travel_TP1_consumption', 'TP1 rate kWh/100 km','TP2 rate kWh/100 km', 'TP2 consumption kWh','TP3 rate kWh/100 km', 'TP3 consumption kWh', 'TP4 rate kWh/100 km','TP4 consumption kWh',\n",
    "\n",
    "\n",
    "d = df[(df['park_end_time']>=day_start_ts) & (df['arr_time']<day_end_ts)].copy()\n",
    "d['hourly_time_dict'] = d.apply(lambda x:create_dict(x.arr_time,x.park_end_time,x.arr_time_idx,x.park_end_time_idx), axis =1)\n",
    "d['charge_time_list'] = d.apply(lambda x:create_charge_time_list(x.c,x.st_chg_time,x.ed_chg_time,x.st_chg_time_idx,x.ed_chg_time_idx), axis=1)\n",
    "d['charge_power_list'] = d.apply(lambda x: [x['chg rate'] if t >30 else 0 for t in x['charge_time_list']], axis=1)\n",
    "d['charge_power_list_sanity'] = d.apply(lambda x: [x['chg rate'] if t >0 else 0 for t in x['charge_time_list']], axis=1)\n",
    "d['charge_energy_list'] = d.apply(lambda x:[t/60*x['chg rate']for t in x['charge_time_list']], axis=1)\n",
    "d['charge_energy_sum'] = d['charge_energy_list'].apply(sum)\n",
    "# d['is_first'] = ~d.duplicated(subset='person', keep='first')\n",
    "# d['is_first'] = d['is_first'].astype(bool)\n",
    "d['augmented_SoE_bc'] = np.where(\n",
    "    (d['st_chg_time'] < day_start_ts) & (d['ed_chg_time'] > day_start_ts),\n",
    "    d['SoE_ac'] - d['charge_energy_sum'],\n",
    "    np.where(\n",
    "        (d['st_chg_time'] < day_start_ts) & (d['ed_chg_time'] < day_start_ts),\n",
    "        d['SoE_ac'],\n",
    "        d['SoE_bc']\n",
    "    )\n",
    ")\n",
    "d['soe_init'] = d.apply(lambda x:get_soe_init(x.augmented_SoE_bc,x.arr_time_idx), axis=1)\n",
    "d['real_chg_e'] = d['SoE_ac']-d['augmented_SoE_bc']\n",
    "d.insert(0,'event_index',d.index)\n",
    "\n",
    "d.to_pickle(f'{path}/grid_{grid}_{scenario_year}_{day_start_ts}_{monitor_hr}_preprocessed_2trip.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b6e018b7e0eff8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-16T21:03:40.881691Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(f'{path}/grid_{grid}_{scenario_year}_{day_start_ts}_{monitor_hr}_preprocessed_2trip.pkl','rb') as d:\n",
    "    d = pickle.load(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a7f258c8db5b0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-16T21:03:40.883652Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 1: Calculate aggregates for arr_hour and dep_hour for each person_id\n",
    "times = d.groupby('person').agg({\n",
    "    'arr_time_idx':['min','max'],\n",
    "    'dep_time_idx':['min','max']\n",
    "}).reset_index()\n",
    "# Flatten the MultiIndex for columns created by aggregation\n",
    "times.columns = ['_'.join(col).strip('_') for col in times.columns.values]\n",
    "# Create a feature dataframe for clustering\n",
    "feature_df = times[['arr_time_idx_min', 'arr_time_idx_min', 'dep_time_idx_min', 'dep_time_idx_max']]\n",
    "\n",
    "# Step 2: Cluster based on behavior\n",
    "kmeans = KMeans(n_clusters=30, random_state=42).fit(feature_df)\n",
    "times['cluster'] = kmeans.labels_\n",
    "# Assign cluster labels to each person\n",
    "clustered = pd.merge(d, times[['person', 'cluster']], on='person')\n",
    "clustered = clustered.sort_values(by=['person','dep_time_idx','cluster'])\n",
    "# Get unique persons and the start index for each person\n",
    "unique_persons = clustered['person'].unique()\n",
    "person_start_index = {person: i for i, person in enumerate(unique_persons)}\n",
    "\n",
    "\n",
    "# Create a dictionary to hold the group number for each person\n",
    "person_to_group = {}\n",
    "# Assign groups in a round-robin fashion\n",
    "group_number = 1\n",
    "for person in unique_persons:\n",
    "    person_to_group[person] = group_number\n",
    "    group_number = group_number % num_groups + 1  # Loop back to 1 after reaching num_groups\n",
    "# Now assign the group to each row in the DataFrame based on the 'person' column\n",
    "clustered['group'] = clustered['person'].map(person_to_group)\n",
    "clustered.drop(['cluster'],axis=1,inplace=True)\n",
    "clustered['pre_SoE_ac'] = clustered.groupby(['group','person', 'grid'],dropna=False)['SoE_ac'].shift(1).fillna(clustered['augmented_SoE_bc'])\n",
    "clustered['SoE_change'] = clustered['pre_SoE_ac']-clustered['augmented_SoE_bc']\n",
    "clustered['next_dep_time_idx'] = clustered.groupby('person')['dep_time_idx'].shift(-1).fillna(monitor_hr-1)\n",
    "clustered['next_SoE_change'] = clustered.groupby('person')['SoE_change'].shift(-1).fillna(0)\n",
    "\n",
    "clustered.drop(['pre_SoE_ac'],axis=1)\n",
    "clustered.set_index('event_index', inplace=True)\n",
    "clustered.to_pickle(f'{path}/grid_{grid}_{scenario_year}_{day_start_ts}_{monitor_hr}_2trip_clustered_{num_groups}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd884b694cfe4168",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-16T21:03:40.886035Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize Tobia's Nexus Output\n",
    "hv_bus = str(89)\n",
    "# Controlled charging\n",
    "charge = pd.read_csv(f\"/Users/huiwen/Library/Mobile Documents/com~apple~CloudDocs/Thesis/extracted_data/map_bus/Added_up_charge_{scenario_year}_raw.csv\") # in MW\n",
    "charge['ts'] = pd.to_datetime(charge['ts'])\n",
    "charge = charge.loc[(charge.ts<day_end_ts) & (charge.ts>=day_start_ts)][['ts','peak',hv_bus]]\n",
    "# Controlled discharging\n",
    "discharge = pd.read_csv(f\"/Users/huiwen/Library/Mobile Documents/com~apple~CloudDocs/Thesis/extracted_data/map_bus/EVBatt_power_hourly_{scenario_year}_discharge_mapped.csv\")\n",
    "discharge = discharge.rename(columns={'Unnamed: 0':'ts'})\n",
    "discharge['ts'] = pd.to_datetime(discharge['ts'])\n",
    "discharge = discharge.loc[(discharge.ts<day_end_ts) & (discharge.ts>=day_start_ts)][['ts',hv_bus]]\n",
    "# Find Netload Max\n",
    "net= charge['89']-discharge['89']\n",
    "period_max = net.max()\n",
    "charge['normalized_profile'] = charge[hv_bus]/period_max\n",
    "charge.index=range(monitor_hr)\n",
    "discharge['normalized_profile'] = discharge[hv_bus]/period_max\n",
    "discharge.index=range(monitor_hr)\n",
    "net = charge['89']-discharge['89']\n",
    "net_normalized = net/period_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6445b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_matching(i,path):\n",
    "    m = pyo.ConcreteModel()\n",
    "\n",
    "    ## Functions\n",
    "    def initialize_max_p_rule(m,park,person,arr_idx,end_idx):\n",
    "        return MAX_P_dict[park]\n",
    "    def initialize_soe_change_rule(m,park,person,arr_idx,end_idx):\n",
    "        return SOE_CHANGE_dict[park]\n",
    "    def p_bound_rule(m,park,person,arr_idx,end_idx,t):\n",
    "        if (park,person,arr_idx,end_idx,t) not in PARKHR_dict.keys() or (PARKHR_dict[(park,person,arr_idx,end_idx,t)]<=0):\n",
    "            return (0,0)\n",
    "        else:\n",
    "            return (0,MAX_P_dict[park])\n",
    "    def initialize_next_e_rule(m,park,person,arr_idx,end_idx):\n",
    "        return NEXT_E_dict[park]\n",
    "    def soe_bound_rule(m,person,t):\n",
    "        return (0,EVCAP_dict[person])\n",
    "    def soe_init_rule(m,person,t):\n",
    "        if t==0:\n",
    "            return SOE_init_dict[person]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    ## Const\n",
    "    M = 1e8\n",
    "    PARK_CNT = len(cluster)\n",
    "    unshifted_daily_net_charge = normalized_tot_e\n",
    "\n",
    "    ## Set\n",
    "    m.EVBAT = pyo.Set(initialize=cluster.person.unique())\n",
    "    m.PARK = pyo.Set(dimen=4,initialize = park_tuple) # (Parking Event,EVBAT,Bus) Set\n",
    "    m.T = pyo.Set(initialize=list(range(monitor_hr)))\n",
    "\n",
    "    ## Params\n",
    "    m.CHARGE_TARGET = pyo.Param(m.T,initialize=charge_to_match_dict)\n",
    "    m.DISCHARGE_TARGET = pyo.Param(m.T,initialize=discharge_to_match_dict)\n",
    "    m.SOE_init = pyo.Param(m.EVBAT,initialize=SOE_init_dict)\n",
    "    m.MAX_P = pyo.Param(m.PARK,initialize=initialize_max_p_rule)\n",
    "    m.SOE_CHANGE = pyo.Param(m.PARK,initialize=initialize_soe_change_rule)\n",
    "    m.PARKHR = pyo.Param(m.PARK,m.T,initialize=PARKHR_dict,default=0)\n",
    "    m.PPLANED = pyo.Param(m.PARK,m.T,initialize=P_PLANED_dict,default=0)\n",
    "    m.EVCAP = pyo.Param(m.EVBAT,initialize=EVCAP_dict)\n",
    "    m.NEXT_E = pyo.Param(m.PARK,initialize=initialize_next_e_rule)\n",
    "\n",
    "    ## Vars\n",
    "    m.SOE = pyo.Var(m.EVBAT,m.T,within=pyo.NonNegativeReals,bounds=soe_bound_rule,initialize=soe_init_rule)#\n",
    "    m.CHARGE_P = pyo.Var(m.PARK, m.T,within=pyo.NonNegativeReals,bounds=p_bound_rule,initialize=0) \n",
    "    m.DISCHARGE_P = pyo.Var(m.PARK,m.T,within=pyo.NonNegativeReals,bounds=p_bound_rule,initialize=0)\n",
    "    m.IS_CHARGING = pyo.Var(m.PARK,m.T,within=pyo.Binary,initialize=0)\n",
    "    m.IS_DISCHARGING = pyo.Var(m.PARK,m.T,within=pyo.Binary,initialize=0)\n",
    "    m.IS_ACTIVE = pyo.Var(m.PARK,within=pyo.Binary,initialize=0)\n",
    "    m.CHARGE_JUMP = pyo.Var(m.PARK,m.T, within=pyo.Binary,initialize=0) # detect charge jump\n",
    "    m.DISCHARGE_JUMP = pyo.Var(m.PARK,m.T, within=pyo.Binary,initialize=0) # detect discharge jump\n",
    "    m.FLEX = pyo.Var(m.PARK,m.T, within=pyo.Binary,initialize=0) # detect flexibility for each person\n",
    "\n",
    "    ## Slack/Auxiliary\n",
    "    m.POWER_SHIFT_ABS = pyo.Var(m.PARK,m.T,within=pyo.NonNegativeReals)\n",
    "    m.BUFFER = pyo.Var(m.T,within=pyo.NonNegativeReals,initialize=0)\n",
    "    m.ENERGY_DEFICIT = pyo.Var(within=pyo.NonNegativeReals,bounds=(0,0.3*unshifted_daily_net_charge),initialize=0) #Room for daily net charge energy to deviate\n",
    "    m.ENERGY_SURPLUS = pyo.Var(within=pyo.NonNegativeReals,bounds=(0,0.3*unshifted_daily_net_charge),initialize=0) #Room for daily net charge energy to deviate\n",
    "    m.TOT_CHARGE = pyo.Var(m.T,within=pyo.NonNegativeReals,initialize=0) #Auxiliary Variable for hourly tot charge power\n",
    "    m.TOT_DISCHARGE = pyo.Var(m.T,within=pyo.NonNegativeReals,initialize=0) #Auxiliary Variable for hourly tot discharge power\n",
    "\n",
    "\n",
    "    ############################\n",
    "    def identify_charging_rule(m,park,person,arr_idx,end_idx,t):\n",
    "        \"\"\"\n",
    "        enforce is_charging=1 if charging, 0 if discharging/no action\n",
    "        \"\"\"\n",
    "        return m.CHARGE_P[park,person,arr_idx,end_idx,t] <= M * m.IS_CHARGING[park,person,arr_idx,end_idx,t]\n",
    "    m.IdentifyCharging = pyo.Constraint(m.PARK, m.T, rule=identify_charging_rule)\n",
    "\n",
    "\n",
    "    def identify_discharging_rule(m,park,person,arr_idx,end_idx,t):\n",
    "        \"\"\"\n",
    "        enforce is_discharging=1 if discharging, 0 if charging/no action\n",
    "        \"\"\"\n",
    "        return m.DISCHARGE_P[park,person,arr_idx,end_idx,t] <= M * m.IS_DISCHARGING[park,person,arr_idx,end_idx,t]\n",
    "    m.IdentifyDischarging = pyo.Constraint(m.PARK, m.T, rule=identify_discharging_rule)\n",
    "\n",
    "\n",
    "    def non_simultaneous_rule(m,park,person,arr_idx,end_idx,t):\n",
    "        \"\"\"\n",
    "        Avoid simultaneous charge and discharge\n",
    "        \"\"\"\n",
    "        return m.IS_CHARGING[park,person,arr_idx,end_idx,t] + m.IS_DISCHARGING[park,person,arr_idx,end_idx,t] <= 1\n",
    "    m.NonSimultaneous = pyo.Constraint(m.PARK, m.T, rule=non_simultaneous_rule)\n",
    "\n",
    "\n",
    "    def decreasing_charge_rule(m,park,person,arr_idx,end_idx,t):\n",
    "        \"\"\"\n",
    "        Charging power monotonically decreasing\n",
    "        \"\"\"\n",
    "        if t>=arr_idx and t<(monitor_hr-1):\n",
    "            return (m.CHARGE_P[park,person,arr_idx,end_idx,t]-m.CHARGE_P[park,person,arr_idx,end_idx,t+1])*m.IS_CHARGING[park,person,arr_idx,end_idx,t]>=0\n",
    "        else:\n",
    "            return pyo.Constraint.Skip\n",
    "    m.DecreaseCharging = pyo.Constraint(m.PARK,m.T,rule=decreasing_charge_rule)\n",
    "\n",
    "\n",
    "    def decreasing_discharge_rule(m,park,person,arr_idx,end_idx,t):\n",
    "        \"\"\"\n",
    "        Discharging power monotonically decreasing\n",
    "        \"\"\"\n",
    "        if t>=arr_idx and t<(monitor_hr-1):\n",
    "            return (m.DISCHARGE_P[park,person,arr_idx,end_idx,t]-m.DISCHARGE_P[park,person,arr_idx,end_idx,t+1])*m.IS_DISCHARGING[park,person,arr_idx,end_idx,t]>=0\n",
    "        else:\n",
    "            return pyo.Constraint.Skip\n",
    "    m.DecreaseDischarging = pyo.Constraint(m.PARK,m.T,rule=decreasing_discharge_rule)\n",
    "\n",
    "    def soe_update_rule(m,person,t):\n",
    "        \"\"\"\n",
    "        Update SOE \n",
    "        \"\"\"\n",
    "        if t==0:\n",
    "            return m.SOE[person,0]==m.SOE_init[person]\n",
    "        else:\n",
    "            return m.SOE[person,t]==(m.SOE[person,t-1]\n",
    "                                    +sum(m.CHARGE_P[park,person,arr_idx,end_idx,t-1]*m.PARKHR[park,person,arr_idx,end_idx,t-1] for park,p,arr_idx,end_idx in m.PARK if p==person)\n",
    "                                    -sum(m.DISCHARGE_P[park,person,arr_idx,end_idx,t-1]*m.PARKHR[park,person,arr_idx,end_idx,t-1] for park,p,arr_idx,end_idx in m.PARK if p==person)\n",
    "                                    -sum(m.SOE_CHANGE[park,p,arr_idx,end_idx] for park,p,arr_idx,end_idx in m.PARK if (p==person and arr_idx==t))\n",
    "                                    )\n",
    "    m.SoeUpdate = pyo.Constraint(m.EVBAT,m.T,rule=soe_update_rule)\n",
    "\n",
    "    def soe_end_rule(m,person):\n",
    "        return (m.SOE[person,monitor_hr-1]\n",
    "                +sum(m.CHARGE_P[park,person,arr_idx,end_idx,monitor_hr-1]*m.PARKHR[park,person,arr_idx,end_idx,monitor_hr-1] for park,p,arr_idx,end_idx in m.PARK if p==person)\n",
    "                -sum(m.DISCHARGE_P[park,person,arr_idx,end_idx,monitor_hr-1]*m.PARKHR[park,person,arr_idx,end_idx,monitor_hr-1] for park,p,arr_idx,end_idx in m.PARK if p==person)\n",
    "                )<=m.EVCAP[person]\n",
    "    m.SoeEnd = pyo.Constraint(m.EVBAT,rule=soe_end_rule)\n",
    "\n",
    "    # def soe_upper_rule(m,person,t):\n",
    "    #     \"\"\"\n",
    "    #     SOE upper soft limit\n",
    "    #     \"\"\"\n",
    "    #     return m.SOE[person,t]<=m.EVCAP[person]\n",
    "    # m.SoeUpper = pyo.Constraint(m.EVBAT,m.T,rule=soe_upper_rule)\n",
    "\n",
    "    def next_trip_rule(m,park,person,arr_idx,end_idx,t):\n",
    "        \"\"\"\n",
    "        SOE requirement for next trip\n",
    "        \"\"\"\n",
    "        if end_idx==t-1:\n",
    "            return m.SOE[person,t]>=m.NEXT_E[park,person,arr_idx,end_idx]\n",
    "        else:\n",
    "            return pyo.Constraint.Skip  \n",
    "    m.NextTrip = pyo.Constraint(m.PARK,m.T,rule=next_trip_rule)\n",
    "\n",
    "    def detect_charge_jump(m,park,person,arr_idx,end_idx,t):\n",
    "        \"\"\"\n",
    "        Detect start of charging process\n",
    "        \"\"\"\n",
    "        if t==0:\n",
    "            return m.CHARGE_JUMP[park,person,arr_idx,end_idx,t]==1*m.IS_CHARGING[park,person,arr_idx,end_idx,t]\n",
    "        else:\n",
    "            return m.IS_CHARGING[park,person,arr_idx,end_idx,t]-m.IS_CHARGING[park,person,arr_idx,end_idx,t-1] <= M * m.CHARGE_JUMP[park,person,arr_idx,end_idx,t]\n",
    "    m.DetectChargeJump = pyo.Constraint(m.PARK,m.T,rule=detect_charge_jump)\n",
    "\n",
    "    def charge_jump_rule(m,park,p,arr_idx,end_idx):\n",
    "        \"\"\"\n",
    "        max. 1 start of charging is allowed\n",
    "        \"\"\"\n",
    "        return sum(m.CHARGE_JUMP[park,p,arr_idx,end_idx,t] for t in m.T )<=1\n",
    "    m.ChargeJump = pyo.Constraint(m.PARK,rule=charge_jump_rule)\n",
    "\n",
    "    def detect_discharge_jump(m,park,person,arr_idx,end_idx,t):\n",
    "        \"\"\"\n",
    "        Detect start of charging process\n",
    "        \"\"\"\n",
    "        if t==0:\n",
    "            return m.DISCHARGE_JUMP[park,person,arr_idx,end_idx,t]==1*m.IS_DISCHARGING[park,person,arr_idx,end_idx,t]\n",
    "        else:\n",
    "            return m.IS_DISCHARGING[park,person,arr_idx,end_idx,t]-m.IS_DISCHARGING[park,person,arr_idx,end_idx,t-1] <= M * m.DISCHARGE_JUMP[park,person,arr_idx,end_idx,t]\n",
    "    m.DetectDischargeJump = pyo.Constraint(m.PARK,m.T,rule=detect_discharge_jump)\n",
    "\n",
    "    def discharge_jump_rule(m,park,p,arr_idx,end_idx):\n",
    "        \"\"\"\n",
    "        max. 1 start of discharging is allowed\n",
    "        \"\"\"\n",
    "        return sum(m.DISCHARGE_JUMP[park,p,arr_idx,end_idx,t] for t in m.T )<=1\n",
    "    m.DishargeJump = pyo.Constraint(m.PARK,rule=discharge_jump_rule)\n",
    "\n",
    "    def only_charge_or_discharge_rule(m,park,p,arr_idx,end_idx):\n",
    "        \"\"\"\n",
    "        Only charge or discharge is allowed for each parking event\n",
    "        \"\"\"\n",
    "        return sum(m.CHARGE_JUMP[park,p,arr_idx,end_idx,t] for t in m.T )+sum(m.DISCHARGE_JUMP[park,p,arr_idx,end_idx,t] for t in m.T )<= 1\n",
    "    m.OneJump = pyo.Constraint(m.PARK,rule=only_charge_or_discharge_rule)\n",
    "\n",
    "\n",
    "    def min_charge_time_rule(m,park,p,arr_idx,end_idx):\n",
    "        \"\"\"\n",
    "        Be active for at least 1 hr\n",
    "        \"\"\"\n",
    "        return sum(m.IS_CHARGING[park,p,arr_idx,end_idx,t] * m.PARKHR[park,p,arr_idx,end_idx,t] + m.IS_DISCHARGING[park,p,arr_idx,end_idx,t] * m.PARKHR[park,p,arr_idx,end_idx,t] for t in m.T) >= 1 * m.IS_ACTIVE[park,p,arr_idx,end_idx]\n",
    "    m.MinActiveDuration = pyo.Constraint(m.PARK, rule=min_charge_time_rule)\n",
    "\n",
    "    def activity_rule(m,park,p,arr_idx,end_idx):\n",
    "        \"\"\"\n",
    "        Identify wether there's action decided in a parking event\n",
    "        \"\"\"\n",
    "        return sum(m.IS_CHARGING[park,p,arr_idx,end_idx,t]*m.PARKHR[park,p,arr_idx,end_idx,t]+m.IS_DISCHARGING[park,p,arr_idx,end_idx,t]*m.PARKHR[park,p,arr_idx,end_idx,t] for t in m.T)<=M*m.IS_ACTIVE[park,p,arr_idx,end_idx]\n",
    "    m.DetectAcitivity = pyo.Constraint(m.PARK,rule=activity_rule)\n",
    "\n",
    "    def power_shift_rule_1(m,park,p,arr_idx,end_idx,t):\n",
    "        \"\"\"\n",
    "        Create the slack m.POWER_SHIFT_ABS to identify wether the charging is shfited\n",
    "        \"\"\"\n",
    "        return m.POWER_SHIFT_ABS[park,p,arr_idx,end_idx,t]>=(m.CHARGE_P[park,p,arr_idx,end_idx,t]-m.DISCHARGE_P[park,p,arr_idx,end_idx,t])-m.PPLANED[park,p,arr_idx,end_idx,t]\n",
    "    m.PowerShiftAbs1 = pyo.Constraint(m.PARK,m.T,rule=power_shift_rule_1)\n",
    "        \n",
    "    def power_shift_rule_2(m,park,p,arr_idx,end_idx,t):\n",
    "        \"\"\"\n",
    "        Create the slack m.POWER_SHIFT_ABS to identify wether the charging is shfited\n",
    "        \"\"\"\n",
    "        return m.POWER_SHIFT_ABS[park,p,arr_idx,end_idx,t]>=-((m.CHARGE_P[park,p,arr_idx,end_idx,t]-m.DISCHARGE_P[park,p,arr_idx,end_idx,t])-m.PPLANED[park,p,arr_idx,end_idx,t])\n",
    "    m.PowerShiftAbs2 = pyo.Constraint(m.PARK,m.T,rule=power_shift_rule_2)\n",
    "\n",
    "    def plug_in_cnt_rule(m,t):\n",
    "        \"\"\"\n",
    "        Count plug in number at each m.T\n",
    "        \"\"\"\n",
    "        return sum(1 if m.PARKHR[park,person,arr_idx,end_idx,t]>0 else 0 for park,person,arr_idx,end_idx in m.PARK)\n",
    "    m.PlugInCnt = pyo.Expression(m.T,rule=plug_in_cnt_rule)\n",
    "\n",
    "    def decide_flex_rule(m,park,person,arr_idx,end_idx,t):\n",
    "        \"\"\"\n",
    "        Determine value for m.FLEX\n",
    "        \"\"\"\n",
    "        return m.POWER_SHIFT_ABS[park,person,arr_idx,end_idx,t] <= M*m.FLEX[park,person,arr_idx,end_idx,t]\n",
    "    m.DecideFlex = pyo.Constraint(m.PARK,m.T,rule=decide_flex_rule)\n",
    "\n",
    "    def participate_rule(m,t):\n",
    "        \"\"\"\n",
    "        Flexibility particiaption limit\n",
    "        \"\"\"\n",
    "        return sum(m.FLEX[park,person,arr_idx,end_idx,t] for park,person,arr_idx,end_idx in m.PARK)<=0.29*m.PlugInCnt[t] + m.BUFFER[t]\n",
    "    m.ParticipateLimit = pyo.Constraint(m.T, rule=participate_rule)\n",
    "\n",
    "\n",
    "    def net_charge_daily(m):\n",
    "        \"\"\"\n",
    "        Sum of daily net charging energy\n",
    "        \"\"\"\n",
    "        return sum((m.CHARGE_P[park,person,arr_idx,end_idx,t]-m.DISCHARGE_P[park,person,arr_idx,end_idx,t])*m.PARKHR[park,person,arr_idx,end_idx,t] for park,person,arr_idx,end_idx in m.PARK for t in m.T)\n",
    "    m.NetChargeDaily = pyo.Expression(rule=net_charge_daily)\n",
    "\n",
    "    def match_daily_energy_rule(m):\n",
    "        \"\"\"\n",
    "        Match daily net charge energy as close as possible\n",
    "        \"\"\"\n",
    "        return m.NetChargeDaily + m.ENERGY_DEFICIT == unshifted_daily_net_charge + m.ENERGY_SURPLUS\n",
    "    m.MatchDailyNetCharge = pyo.Constraint(rule=match_daily_energy_rule)\n",
    "\n",
    "    def hourly_charge_power_sum_rule(m,t):\n",
    "        return sum(m.CHARGE_P[park,person,arr_idx,end_idx,t] * m.IS_CHARGING[park,person,arr_idx,end_idx,t] for park,person,arr_idx,end_idx in m.PARK)\n",
    "    m.HourlyChargedPowerSum = pyo.Expression(m.T,rule=hourly_charge_power_sum_rule)\n",
    "    def mask_tot_charge_z(m,t):\n",
    "        return m.TOT_CHARGE[t]==m.HourlyChargedPowerSum[t]\n",
    "    m.TotCharge = pyo.Constraint(m.T, rule=mask_tot_charge_z)\n",
    "\n",
    "    def hourly_discharge_power_sum_rule(m,t):\n",
    "        return sum(m.DISCHARGE_P[park,person,arr_idx,end_idx,t] * m.IS_DISCHARGING[park,person,arr_idx,end_idx,t] for park,person,arr_idx,end_idx in m.PARK)\n",
    "    m.HourlyDischargedPowerSum = pyo.Expression(m.T,rule=hourly_discharge_power_sum_rule)\n",
    "    def mask_tot_discharge_z(m,t):\n",
    "        return m.TOT_DISCHARGE[t]==m.HourlyDischargedPowerSum[t]\n",
    "    m.TotDischarge = pyo.Constraint(m.T, rule=mask_tot_discharge_z)\n",
    "\n",
    "    ###########################\n",
    "    # Objective \n",
    "    def objective_rule(m):\n",
    "        return sum((m.CHARGE_TARGET[t]-m.TOT_CHARGE[t])**2 for t in m.T) + sum((m.DISCHARGE_TARGET[t]-m.TOT_DISCHARGE[t])**2 for t in m.T) + 10000*sum(m.BUFFER[t] for t in m.T)+0.1*sum(m.POWER_SHIFT_ABS[park,person,arr_idx,end_idx,t] for park,person,arr_idx,end_idx in m.PARK for t in m.T)\n",
    "    m.objective = pyo.Objective(rule=objective_rule, sense=pyo.minimize)\n",
    "\n",
    "\n",
    "    ###########################\n",
    "    # Solve model\n",
    "    solver = pyo.SolverFactory('gurobi')\n",
    "    # solver.options['FeasibilityTol'] = 1e-2\n",
    "    # solver.options['OptimalityTol'] = 1e-2\n",
    "    solver.options['MIPGapAbs'] = 0.7\n",
    "    solver.options['NoRelHeurTime'] = 30\n",
    "    solver.options['NodeMethod'] = 2\n",
    "    solver.options['MIPFocus'] = 1\n",
    "    # solver.options['TimeLimit'] = 300\n",
    "    solver.solve(m,tee=True,logfile=f\"{path}/cluster_{i}.log\")   #,keepfiles=True,logfile=\"match_profile_log.log\")\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    log_infeasible_constraints(m)\n",
    "\n",
    "    ###############################\n",
    "    # Save Results\n",
    "    ch_dict = {(park, t): m.CHARGE_P[park,person,arr_idx,end_idx, t].value*emob_max_p for park,person,arr_idx,end_idx in m.PARK for t in m.T} # Denormalize back to normal power value in kW\n",
    "    dis_dict = {(park,t):m.DISCHARGE_P[park,person,arr_idx,end_idx,t].value*emob_max_p for park,person,arr_idx,end_idx in m.PARK for t in m.T}\n",
    "    soe_dict = {(person,t):pyo.value(m.SOE[person,t]*emob_max_p) for person in m.EVBAT for t in m.T}\n",
    "    flex_dict = {(person,t): m.FLEX[park,person,arr_idx,end_idx,t].value for park,person,arr_idx,end_idx in m.PARK for t in m.T}\n",
    "    ch = pd.Series(ch_dict).unstack()\n",
    "    ch.to_csv(f'{path}/{grid}_cluster_{i}_charge.csv')\n",
    "    dis = pd.Series(dis_dict).unstack()\n",
    "    dis.to_csv(f'{path}/{grid}_cluster_{i}_discharge.csv')\n",
    "    soe = pd.Series(soe_dict).unstack()\n",
    "    soe.to_csv(f'{path}/{grid}_cluster_{i}_soe.csv')\n",
    "    person_flag = pd.Series(flex_dict).unstack()\n",
    "    person_flag.to_csv(f\"{path}/{grid}_cluster_{i}_person_participate_flag.csv\")\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675655ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from local result\n",
    "with open(f\"{path}/grid_{grid}_{scenario_year}_{day_start_ts}_{monitor_hr}_2trip_clustered_{num_groups}.pkl\",\"rb\") as c:\n",
    "    clustered = pickle.load(c)\n",
    "clustered = clustered[(clustered['grid']==grid)]\n",
    "\n",
    "for i in range(num_groups):\n",
    "\n",
    "    cluster = clustered[(clustered['group']==i+1) & (clustered['grid']==grid)]\n",
    "    emob_agg_e = [sum(x) for x in zip(*cluster['charge_energy_list'])] # energy in kWh\n",
    "    emob_agg_p = [sum(x) for x in zip(*cluster['charge_power_list'])] # power in kW\n",
    "    emob_max_p, emob_min_p,emob_tot_p = max(emob_agg_p), min(emob_agg_p), sum(emob_agg_p)\n",
    "    emob_max_e, emob_min_e, emob_tot_e= max(emob_agg_e), min(emob_agg_e),sum(emob_agg_e)\n",
    "    normalized_tot_e = emob_tot_e/emob_max_p\n",
    "    print(f\"Start Cluster {i}\")\n",
    "    print(emob_max_p)\n",
    "    if emob_max_p!=0:\n",
    "        \"\"\"\n",
    "        Prepare Model Input Data\n",
    "        \"\"\"\n",
    "        emob_agg_p_norm =  [p/emob_max_p for p in emob_agg_p]\n",
    "        cluster.loc[:,'normalized_chg_power'] = cluster['chg rate']/emob_max_p\n",
    "        \n",
    "        charge_to_match_dict = charge.normalized_profile.to_dict()\n",
    "        discharge_to_match_dict = discharge.normalized_profile.to_dict()\n",
    "\n",
    "        SOE_init_dict = (cluster.groupby('person')['augmented_SoE_bc'].first()/emob_max_p).to_dict()\n",
    "        PARK_to_PERSON_dict = cluster.person.to_dict()\n",
    "        PARK_to_ENDIDX_dict = cluster.park_end_time_idx.to_dict() # key:parking event -> value: park_end_time_idx\n",
    "        PARK_to_ARRIDX_dict = cluster.arr_time_idx.to_dict() # key:parking event -> value: arr_time_idx\n",
    "        park_tuple = [(PARK,int(PARK_to_PERSON_dict[PARK]),max(PARK_to_ARRIDX_dict[PARK],0),min(int(PARK_to_ENDIDX_dict[PARK]),monitor_hr-1)) for PARK,PERSON in  PARK_to_PERSON_dict.items()]\n",
    "        MAX_P_dict = (cluster['chg rate']/emob_max_p).to_dict()\n",
    "        SOE_CHANGE_dict = (cluster.SoE_change/emob_max_p).to_dict()\n",
    "        NEXT_E_dict = (cluster.next_trip_e/emob_max_p).to_dict()\n",
    "        PARKHR_dict = {(PARK,int(PARK_to_PERSON_dict[PARK]),max(PARK_to_ARRIDX_dict[PARK],0),min(int(PARK_to_ENDIDX_dict[PARK]),monitor_hr-1),t):cluster.loc[PARK].hourly_time_dict[t]/60 for PARK in cluster.index for t in range(monitor_hr)}\n",
    "        EVCAP_dict = (cluster.groupby('person').B.first()/emob_max_p).to_dict()\n",
    "        \n",
    "        P_PLANED_coo = create_sparse_dict(cluster.charge_power_list_sanity,monitor_hr)\n",
    "        P_PLANED_dict = {(park,int(PARK_to_PERSON_dict[park]),max(PARK_to_ARRIDX_dict[park],0),min(int(PARK_to_ENDIDX_dict[park]),monitor_hr-1),t):power/emob_max_p for (park,t,power) in zip(P_PLANED_coo.row,P_PLANED_coo.col,P_PLANED_coo.data)}\n",
    "        PARKHR_coo = create_sparse_dict(cluster.hourly_time_dict,monitor_hr)\n",
    "        PARKHR_dict = {(park,int(PARK_to_PERSON_dict[park]),max(PARK_to_ARRIDX_dict[park],0),min(int(PARK_to_ENDIDX_dict[park]),monitor_hr-1),t):minutes/60 for  park,t,minutes in zip(PARKHR_coo.row,PARKHR_coo.col,PARKHR_coo.data)}\n",
    "\n",
    "        opt_res_code = shape_matching(i,path)\n",
    "    else:\n",
    "        index = list(cluster.index)\n",
    "        T = list(range(monitor_hr))\n",
    "        res_dict = {(e, t):0 for e in index for t in T} # Denormalize back to normal power value in kW\n",
    "        # Convert the dictionary into a multi-index series to facilitate unstacking\n",
    "        res = pd.Series(res_dict).unstack()\n",
    "        # Save the restructured data to CSV\n",
    "        res.to_csv(f'{path}/369_0_cluster_{i}.csv')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b908252e12a958",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T20:57:46.009377Z",
     "start_time": "2024-04-16T20:57:45.998766Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agg_e = [sum(x) for x in zip(*clustered['charge_energy_list'])] # energy in kWh\n",
    "agg_p = [sum(x) for x in zip(*clustered['charge_power_list'])] # power in kW\n",
    "\n",
    "max_p,min_p,tot_p = max(agg_p), min(agg_p),sum(agg_p)\n",
    "max_e,min_e,tot_e= max(agg_e), min(agg_e),sum(agg_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897561ca1aabe38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T20:57:55.420260Z",
     "start_time": "2024-04-16T20:57:54.628741Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concat_charge = pd.DataFrame()\n",
    "concat_discharge = pd.DataFrame()\n",
    "concat_soe = pd.DataFrame()\n",
    "concat_flexperson = pd.DataFrame()\n",
    "for i in range(num_groups):\n",
    "    charge_i = pd.read_csv(f'{path}/{grid}_cluster_{i}_charge.csv')\n",
    "    concat_charge = pd.concat([concat_charge,charge_i],axis=0)\n",
    "    discharge_i = pd.read_csv(f'{path}/{grid}_cluster_{i}_discharge.csv')\n",
    "    concat_discharge = pd.concat([concat_discharge,discharge_i],axis=0)\n",
    "    # soe_i = pd.read_csv(f'{path}/{grid}_cluster_{i}_soe.csv')\n",
    "    # concat_soe = pd.concat([concat_soe,soe_i],axis=0)\n",
    "    # flexperson_i = pd.read_csv(f'{path}/{grid}_cluster_{i}_person_participate_flag.csv')\n",
    "    # concat_flexperson = pd.concat([concat_flexperson,flexperson_i],axis=0)\n",
    "    \n",
    "concat_charge = concat_charge.rename(columns={'Unnamed: 0':'event_index'})\n",
    "concat_discharge = concat_discharge.rename(columns={'Unnamed: 0':'event_index'})\n",
    "concat_soe = concat_soe.rename(columns={'Unnamed: 0':'person'})\n",
    "# concat_flexperson = concat_flexperson.rename(columns={'Unnamed: 0':'person'})\n",
    "\n",
    "concat_res = concat_charge-concat_discharge\n",
    "concat_res.loc[:,'event_index'] = concat_charge['event_index']\n",
    "concat_charge.set_index('event_index',inplace=True)\n",
    "concat_discharge.set_index('event_index',inplace=True)\n",
    "concat_res.set_index('event_index', inplace=True)\n",
    "\n",
    "\n",
    "# Cast to 0 is power too low\n",
    "for column in concat_res.columns:\n",
    "    if column != 'event_index':\n",
    "        # Apply the condition and replace values\n",
    "        concat_res.loc[:, column] = concat_res[column].apply(lambda x: 0 if abs(x) < 0.01 else x)\n",
    "        \n",
    "concat_charge.to_csv(f'{path}/concat_charge_power_all.csv')\n",
    "concat_discharge.to_csv(f'{path}/concat_dicharge_power_all.csv')\n",
    "concat_res.to_csv(f'{path}/concat_net_power_all.csv')\n",
    "concat_soe.to_csv(f'{path}/concat_soe_all.csv')\n",
    "concat_flexperson.to_csv(f'{path}/concat_flexperson_all.csv')\n",
    "\n",
    "concat_sum = concat_res.sum().sum()\n",
    "concat_max = concat_res.sum().max()\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Optimized results with solid lines and markers\n",
    "plt.plot(concat_res.sum()/concat_max, label='Optimized Net Charge Normalized', color='blue', linestyle='-', marker='o')\n",
    "plt.plot(concat_charge.sum()/concat_max, label='Optimized Charge Normalized', color='green', linestyle='-', marker='^')\n",
    "plt.plot(concat_discharge.sum()/concat_max, label='Optimized Discharge Normalized', color='red', linestyle='-', marker='s')\n",
    "\n",
    "# Nexus outputs with dashed lines\n",
    "plt.plot(net_normalized, label='Nexus Net Charge Normalized', color='blue', linestyle='--')\n",
    "plt.plot(charge['normalized_profile'], label='Nexus Charge Normalized', color='green', linestyle='--')\n",
    "plt.plot(discharge['normalized_profile'], label='Nexus Discharge Normalized', color='red', linestyle='--')\n",
    "\n",
    "plt.title('Comparison of Nexus Outputs and Optimization Results')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Normalized Value')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{path}/optimized_norm_power_vs_nexus.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd86ee33f16fdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T20:37:31.400038Z",
     "start_time": "2024-04-16T20:37:31.131691Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res_p = concat_res.sum()\n",
    "res_c = concat_charge.sum()\n",
    "res_d = concat_discharge.sum()\n",
    "plt.plot(res_p,label='optimized_net_power_profile')\n",
    "plt.plot(agg_p, label='mobility_data_power_profile')\n",
    "plt.plot(res_c, label='optimized_charge_profile')\n",
    "plt.plot(res_d,label='optimized_discharge_profile')\n",
    "plt.legend()\n",
    "plt.savefig(f'{path}/optimized_vs_mobility_power_profile.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e9f46ceb7761b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T20:37:54.129009Z",
     "start_time": "2024-04-16T20:37:47.361807Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concat_res = concat_res.sort_index()\n",
    "concat_charge = concat_charge.sort_index()\n",
    "concat_discharge = concat_discharge.sort_index()\n",
    "time = pd.DataFrame(clustered['hourly_time_dict'].sort_index())\n",
    "event_e = concat_res.apply(lambda row: [row[col]*time.at[row.name,'hourly_time_dict'][col]/60 for col in range(monitor_hr)],axis=1)\n",
    "charge_e = concat_charge.apply(lambda row: [row[col]*time.at[row.name,'hourly_time_dict'][col]/60 for col in range(monitor_hr)],axis=1)\n",
    "discharge_e = concat_discharge.apply(lambda row: [row[col]*time.at[row.name,'hourly_time_dict'][col]/60 for col in range(monitor_hr)],axis=1)\n",
    "\n",
    "net_e = pd.DataFrame(event_e.tolist())\n",
    "net_e.set_index(concat_res.index,inplace=True)\n",
    "res_net_e = pd.Series(net_e.sum(),name='agg_net_e')\n",
    "res_net_e_sum = res_net_e.sum()\n",
    "\n",
    "charge_e = pd.DataFrame(charge_e.tolist())\n",
    "charge_e.set_index(concat_charge.index,inplace=True)\n",
    "res_charge_e = pd.Series(charge_e.sum(),name='agg_charge_e')\n",
    "res_charge_e_sum = res_charge_e.sum()\n",
    "\n",
    "discharge_e = pd.DataFrame(discharge_e.tolist())\n",
    "discharge_e.set_index(concat_discharge.index,inplace=True)\n",
    "res_discharge_e = pd.Series(discharge_e.sum(),name='agg_charge_e')\n",
    "res_discharge_e_sum = res_discharge_e.sum()\n",
    "\n",
    "\n",
    "plt.plot(res_net_e,label='optimized_net_energy_profile')\n",
    "# plt.plot(res_charge_e, label='optimized_charge_energy_profile')\n",
    "# plt.plot(res_discharge_e, label='optimized_discharge_energy_profile')\n",
    "plt.plot(agg_e,label='mobility_data_energy_profile')\n",
    "\n",
    "plt.text(5,6000,f'mobility:{tot_e:.2f}')\n",
    "plt.text(5,5500,f'optimized_net:{res_net_e_sum:.2f}')\n",
    "plt.text(5,5000,f'optimized_charge:{res_charge_e_sum:.2f}')\n",
    "plt.text(5,4500,f'optimized_discharge:{res_discharge_e_sum:.2f}')\n",
    "plt.legend()\n",
    "plt.savefig(f'{path}/optimized_vs_mobility_net_reqeusted_energy_profile.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80d0cfb8686e397",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T20:38:21.244779Z",
     "start_time": "2024-04-16T20:38:21.139013Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = pd.read_csv(f'{path}/concat_net_power_all.csv',index_col='event_index')\n",
    "powerlist = res.apply(lambda row: [round(x,1) for x in row.tolist()], axis=1)\n",
    "powerlist.sort_index()\n",
    "# soe_all = pd.read_csv(f\"{path}/concat_soe_all.csv\", index_col='person')\n",
    "# soe_end = soe_all.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a74aa6541dfe4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T20:38:32.372216Z",
     "start_time": "2024-04-16T20:38:21.255031Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_SoE_bc(group):\n",
    "    if len(group)>1:\n",
    "        # Initialize the first row of 'new_col' with the value from 'column1'\n",
    "        group['update_SoE_bc'].iloc[0] = (group['day_start_SoE'].iloc[0]-group['SoE_change'].iloc[0])\n",
    "        # For subsequent rows, add 'column1' from the previous row and 'column2' from the previous row\n",
    "        for i in range(1, len(group)):\n",
    "            group['update_SoE_bc'].iloc[i] = group['update_SoE_bc'].iloc[i-1] + (group['optimized_charge_energy_sum'].iloc[i-1]-group['SoE_change'].iloc[i])\n",
    "    else:\n",
    "        group['update_SoE_bc'].iloc[0]=group['day_start_SoE'].iloc[0]-group['SoE_change'].iloc[0]\n",
    "    return group\n",
    "\n",
    "\n",
    "d_in = clustered.copy()\n",
    "d_in.sort_index()\n",
    "d_in.loc[:,'optimized_power_list'] = powerlist\n",
    "d_in.loc[:,'optimized_energy_list'] = d_in.apply(lambda row: [row['hourly_time_dict'][x]/60 * row['optimized_power_list'][x] for x in range(monitor_hr)], axis=1)\n",
    "d_in.loc[:,'optimized_charge_energy_sum'] = d_in['optimized_energy_list'].apply(sum)\n",
    "\n",
    "d_in['process_list'] = d_in.apply(lambda row:get_timestamp_pair(row),axis=1)\n",
    "d_in['optimized_process_mean_power'] = d_in['process_list'].apply(lambda x: np.mean(next(iter(x.values())))if len(x)>0 else 0)\n",
    "d_in['augmented_SoE_ac'] = d_in.apply(lambda row:row['augmented_SoE_bc']+row['charge_energy_sum'] if row['ed_chg_time']>=day_end_ts else row['SoE_ac'],axis=1)\n",
    "d_in['augmented_SoE_ac'] =  np.where(\n",
    "    (d_in['st_chg_time'] < day_end_ts) & (d_in['ed_chg_time'] >=day_end_ts),\n",
    "    d_in['augmented_SoE_bc'] + d_in['charge_energy_sum'],\n",
    "    np.where(\n",
    "        (d_in['st_chg_time'] >= day_end_ts) & (d_in['ed_chg_time'] >= day_end_ts),\n",
    "        d_in['augmented_SoE_bc'],\n",
    "        d_in['SoE_ac']\n",
    "    )\n",
    ")\n",
    "d_in['unshifted_day_end_soe'] = d_in.groupby('person')['augmented_SoE_ac'].transform(lambda x: x.iloc[-1])\n",
    "d_in['process_cnt'] = d_in['process_list'].apply(len)\n",
    "d_in['shifted_process_cnt'] = d_in.groupby('person')['process_cnt'].transform(lambda x:x.sum()) # per person\n",
    "d_in['unshifted_process_cnt'] = d_in.groupby('person')['c'].transform(lambda x: x.sum()) # per person\n",
    "d_in['optimized_power_list'] = d_in['optimized_power_list'].apply(lambda x: [round(float(i),1) for i in x])\n",
    "d_in['charge_power_list_sanity'] = d_in['charge_power_list_sanity'].apply(lambda x: [round(float(i),1) for i in x])\n",
    "d_in['flex'] = d_in['optimized_power_list'] != d_in['charge_power_list_sanity'] # per event\n",
    "d_in['outside_base'] = d_in.apply(lambda row:check_outside(row), axis=1) # per event\n",
    "d_in = d_in.sort_values(by=['person', 'dep_time'])\n",
    "d_in.loc[:,'day_start_SoE'] = d_in['person'].map(d_in.groupby('person')['augmented_SoE_bc'].first())\n",
    "d_in['update_SoE_bc'] = pd.NA\n",
    "d_in = d_in.groupby('person', group_keys=False).apply(update_SoE_bc)\n",
    "d_in['update_SoE_ac'] = d_in['update_SoE_bc'] + d_in['optimized_charge_energy_sum']\n",
    "d_in['day_end_soe'] = d_in.groupby('person')['update_SoE_ac'].transform(lambda x:x.iloc[-1])\n",
    "d_in['day_end_soe_shift'] = d_in['day_end_soe']-d_in['unshifted_day_end_soe']\n",
    "d_in['shifted_st_chg_time'] = d_in.apply(lambda row:correct_st_chg_time(row),axis=1)\n",
    "d_in['shifted_ed_chg_time'] = d_in.apply(lambda row:correct_ed_chg_time(row),axis=1)\n",
    "d_in.to_pickle(f'{path}/grid_{grid}_matched_{day_start_ts}_{monitor_hr}_{scenario_year}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a71c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "(d_in.groupby('person')['unshifted_day_end_soe'].first()-d_in.groupby('person')['day_end_soe'].first()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b903b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in['SoE_more'] = d_in['update_SoE_ac']-d_in['B']\n",
    "len(d_in.loc[d_in['SoE_more']>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c7e65aca79a4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T08:40:47.653380Z",
     "start_time": "2024-04-09T08:40:47.355127Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(f\"{path}/grid_{grid}_matched_{day_start_ts}_{monitor_hr}_{scenario_year}.pkl\", \"rb\") as file:\n",
    "    d_in = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27626b632f03fae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T20:38:32.802360Z",
     "start_time": "2024-04-16T20:38:32.372285Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stats\n",
    "# Participation rate\n",
    "flex_d_in = d_in[d_in['flex']==True].copy()\n",
    "print(\"Person/Vehicle count:\", d_in['person'].nunique())\n",
    "print(\"Parking event count:\", len(d_in))\n",
    "print(\"Person/Vehicel participation count:\",flex_d_in['person'].nunique())\n",
    "print(\"Parking event participation count:\",len(flex_d_in))\n",
    "print(\"Flexibility participation rate by person:\", float(flex_d_in['person'].nunique()/d_in['person'].nunique()))\n",
    "print(\"Flexibility participation rate by parking event\", float(len(flex_d_in)/len(d_in)))\n",
    "\n",
    "\n",
    "# Shifted Day End soe within EVs participated in the flexibility\n",
    "flex_d_in['day_end_soe_change'] = flex_d_in['day_end_soe']-flex_d_in['unshifted_day_end_soe']\n",
    "soe_change =  flex_d_in.groupby('person')['day_end_soe_change'].first()\n",
    "max_change,min_change,sum_change_net = soe_change.max(), soe_change.min(), soe_change.sum()\n",
    "print(\"Net Total Day end SoE change:\",sum_change_net)\n",
    "max_bin = (max(abs(max_change),abs(min_change))//10+1)*10\n",
    "soe_change_bins = np.arange(-max_bin, max_bin + 10, 10)\n",
    "\n",
    "# Adjusting the plots to have the boxplot lying down (horizontal) for alignment with the barplot's values\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 6), gridspec_kw={'height_ratios': [1, 2]}, sharex=True)\n",
    "q1, q2, q3 = np.percentile(soe_change, [25, 50, 75])\n",
    "\n",
    "# Horizontal boxplot for 'soe_shift' on the top subplot\n",
    "axs[0].boxplot(soe_change, vert=False, patch_artist=True, positions=[1])\n",
    "axs[0].set_title('Day end SoE shift')\n",
    "axs[0].set_yticks([1])\n",
    "axs[0].set_yticklabels(['Value'])\n",
    "# Adding quartile labels to the boxplot\n",
    "axs[0].text(q1-30, 1.2, f\"Q1: {q1:.2f}\", ha='center', va='center', fontsize=10, color='blue', backgroundcolor='white')\n",
    "axs[0].text(q2, 1.2, f\"Median: {q2:.2f}\", ha='center', va='center', fontsize=10, color='red', backgroundcolor='white')\n",
    "axs[0].text(q3+30, 1.2, f\"Q3: {q3:.2f}\", ha='center', va='center', fontsize=10, color='blue', backgroundcolor='white')\n",
    "\n",
    "# Barplot for frequency data on the bottom subplot, aligned with the boxplot's values\n",
    "counts,edges,bars = axs[1].hist(soe_change, color='skyblue', edgecolor='black',bins=soe_change_bins)\n",
    "# counts,edges,bars = axs[1].hist(soe_shift, color='skyblue', edgecolor='black',bins=20,density=True)\n",
    "axs[1].set_title('Frequency Count for Day End SOE Shift')\n",
    "axs[1].set_xticks(soe_change_bins)\n",
    "\n",
    "axs[1].set_ylabel('Count')\n",
    "plt.bar_label(bars)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{path}/sanitycheck/day_end_soeshift.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ccd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save Flexibility Participation Flag for Following Days\n",
    "# if following_days:\n",
    "#     flex_dict_new = d_in.groupby('person')['flex'].any().astype(int).to_dict()\n",
    "#     # len(list(set(flex_dict.keys())-set(flex_dict_new.keys())))\n",
    "#     flex_dict_update = flex_dict.copy()\n",
    "#     flex_dict_update.update(flex_dict_new)\n",
    "#     with open(f'{path}/flex_dict.json', 'w') as file:\n",
    "#         json.dump(flex_dict_update, file)\n",
    "# else:\n",
    "#     flex_dict = d_in.groupby('person')['flex'].any().astype(int).to_dict()\n",
    "#     with open(f'{path}/flex_dict.json', 'w') as file:\n",
    "#         json.dump(flex_dict,file)\n",
    "\n",
    "# count_of_ones = sum(1 for value in flex_dict_update.values() if value == 1)\n",
    "# print(count_of_ones,len(flex_dict_update))\n",
    "\n",
    "# with open(f'{path}/flex_dict.json', 'w') as file:\n",
    "#     json.dump(flex_dict_update, file)\n",
    "\n",
    "# # Find common keys using set intersection\n",
    "# common_keys = set(flex_dict_new.keys()) & set(flex_dict_update.keys())\n",
    "\n",
    "# # Create a dictionary to see the values from both dictionaries for these keys\n",
    "# common_items = {key: (flex_dict_new[key], flex_dict_update[key]) for key in common_keys}\n",
    "\n",
    "\n",
    "# different_items = {}\n",
    "# for key in flex_dict_update:\n",
    "#     if key in flex_dict_update and flex_dict_update[key] != flex_dict_new[key]:\n",
    "#         different_items[key] = (flex_dict_new[key], flex_dict_update[key])\n",
    "# print(different_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37482fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all = d_in.copy()\n",
    "# Shifted Day End soe within EVs participated in the flexibility\n",
    "all['day_end_soe_change'] = all['day_end_soe']-all['unshifted_day_end_soe']\n",
    "soe_change_all =  all.groupby('person')['day_end_soe_change'].first()\n",
    "max_change_all,min_change_all,sum_change_net_all = soe_change_all.max(), soe_change_all.min(), soe_change_all.sum()\n",
    "print(\"Net Total Day end SoE change:\",sum_change_net)\n",
    "max_bin_all = (max(abs(max_change_all),abs(min_change_all))//10+1)*10\n",
    "soe_change_bins_all = np.arange(-max_bin_all, max_bin_all + 10, 10)\n",
    "\n",
    "# Adjusting the plots to have the boxplot lying down (horizontal) for alignment with the barplot's values\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 6), gridspec_kw={'height_ratios': [1, 2]}, sharex=True)\n",
    "q1, q2, q3 = np.percentile(soe_change, [25, 50, 75])\n",
    "\n",
    "# Horizontal boxplot for 'soe_shift' on the top subplot\n",
    "axs[0].boxplot(soe_change_all, vert=False, patch_artist=True, positions=[1])\n",
    "axs[0].set_title('Day end SoE shift')\n",
    "axs[0].set_yticks([1])\n",
    "axs[0].set_yticklabels(['Value'])\n",
    "# Adding quartile labels to the boxplot\n",
    "axs[0].text(q1-30, 1.2, f\"Q1: {q1:.2f}\", ha='center', va='center', fontsize=10, color='blue', backgroundcolor='white')\n",
    "axs[0].text(q2, 1.2, f\"Median: {q2:.2f}\", ha='center', va='center', fontsize=10, color='red', backgroundcolor='white')\n",
    "axs[0].text(q3+30, 1.2, f\"Q3: {q3:.2f}\", ha='center', va='center', fontsize=10, color='blue', backgroundcolor='white')\n",
    "\n",
    "# Barplot for frequency data on the bottom subplot, aligned with the boxplot's values\n",
    "counts,edges,bars = axs[1].hist(soe_change_all, color='skyblue', edgecolor='black',bins=soe_change_bins_all)\n",
    "# counts,edges,bars = axs[1].hist(soe_shift, color='skyblue', edgecolor='black',bins=20,density=True)\n",
    "axs[1].set_title('Frequency Count for Day End SOE Shift all Vehicles')\n",
    "axs[1].set_xticks(soe_change_bins_all)\n",
    "\n",
    "axs[1].set_ylabel('Count')\n",
    "plt.bar_label(bars)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{path}/sanitycheck/day_end_soeshift_all_vehicles.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d31299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day End SoC change\n",
    "day_end_soc = d_in.groupby('person')[['day_end_soe','unshifted_day_end_soe','B']].first()\n",
    "\n",
    "def calc_2d_hist_percentage(x_data, y_data, bins):\n",
    "    heatmap, xedges, yedges = np.histogram2d(x_data, y_data, bins=[bins, bins])\n",
    "    total_counts = np.sum(heatmap)\n",
    "    percentage_heatmap = (heatmap / total_counts) * 100  # Convert counts to percentage\n",
    "    return percentage_heatmap, xedges, yedges\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(6,6))\n",
    "bins = np.arange(0,110,10)\n",
    "SoC_change_percentage, SoC_xedges, SoC_yedges = calc_2d_hist_percentage(day_end_soc['day_end_soe']/day_end_soc['B']*100, day_end_soc['unshifted_day_end_soe']/day_end_soc['B']*100, bins)\n",
    "cax = axs.pcolormesh(SoC_xedges, SoC_yedges, SoC_change_percentage.T, shading='auto', cmap='viridis')  # Transpose to correct the axis orientation\n",
    "axs.set_title('Shift Effect on Day End SoC of single EV')\n",
    "axs.set_xlabel('Day end SoC after shifting')\n",
    "axs.set_ylabel('Day end SoC without shifting')\n",
    "axs.set_aspect('equal')  # Set aspect ratio to be equal\n",
    "fig.colorbar(cax, ax=axs, label='Percentage (%)')\n",
    "plt.savefig(f\"{path}/sanitycheck/single_ev_day_end_soc_shift.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a466c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flex charge start SoC\n",
    "charge_data = d_in[d_in['optimized_process_mean_power'] > 0].copy()\n",
    "discharge_data = d_in[d_in['optimized_process_mean_power'] < 0].copy()\n",
    "unshifted_charge_data = d_in[d_in['charge_energy_sum'] > 0].copy()\n",
    "\n",
    "bins = np.arange(0, 110, 10)\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(5,9))\n",
    "\n",
    "# Function to calculate 2D histogram data and convert to percentage\n",
    "def calc_2d_hist_percentage(x_data, y_data, bins):\n",
    "    heatmap, xedges, yedges = np.histogram2d(x_data, y_data, bins=[bins, bins])\n",
    "    total_counts = np.sum(heatmap)\n",
    "    percentage_heatmap = (heatmap / total_counts) * 100  # Convert counts to percentage\n",
    "    return percentage_heatmap, xedges, yedges\n",
    "\n",
    "# Calculate 2D histogram percentages\n",
    "charge_percentage, xedges, yedges = calc_2d_hist_percentage(charge_data['update_SoE_ac']/charge_data['B']*100, charge_data['update_SoE_bc']/charge_data['B']*100, bins)\n",
    "discharge_percentage, _, _ = calc_2d_hist_percentage(discharge_data['update_SoE_ac']/discharge_data['B']*100, discharge_data['update_SoE_bc']/discharge_data['B']*100, bins)\n",
    "unshifted_charge_percentage, _, _ = calc_2d_hist_percentage(unshifted_charge_data['SoE_ac']/unshifted_charge_data['B']*100, unshifted_charge_data['SoE_bc']/unshifted_charge_data['B']*100, bins)\n",
    "# Plot heatmaps for unshifted charging\n",
    "cax = axs[0].pcolormesh(xedges, yedges, unshifted_charge_percentage.T, shading='auto', cmap='viridis')  # Transpose to correct the axis orientation\n",
    "axs[0].set_title('SoC for unshifted Charging')\n",
    "axs[0].set_xlabel('State of Charge After (%)')\n",
    "axs[0].set_ylabel('State of Charge Before (%)')\n",
    "axs[0].set_aspect('equal')  # Set aspect ratio to be equal\n",
    "fig.colorbar(cax, ax=axs[0], label='Percentage (%)')\n",
    "\n",
    "# Plot heatmaps for shifted charging\n",
    "cax = axs[1].pcolormesh(xedges, yedges, charge_percentage.T, shading='auto', cmap='viridis')\n",
    "axs[1].set_title('SoC for Shifted Charging')\n",
    "axs[1].set_xlabel('State of Charge After (%)')\n",
    "axs[1].set_ylabel('State of Charge Before (%)')\n",
    "axs[1].set_aspect('equal')\n",
    "fig.colorbar(cax, ax=axs[1], label='Percentage (%)')\n",
    "\n",
    "\n",
    "# Plot heatmaps for shifted discharging\n",
    "cax = axs[2].pcolormesh(xedges, yedges, discharge_percentage.T, shading='auto', cmap='viridis')\n",
    "axs[2].set_title('SoC for Shifted Discharging')\n",
    "axs[2].set_xlabel('State of Charge After (%)')\n",
    "axs[2].set_ylabel('State of Charge Before (%)')\n",
    "axs[2].set_aspect('equal')\n",
    "fig.colorbar(cax, ax=axs[2], label='Percentage (%)')\n",
    "\n",
    "# Layout adjustments\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{path}/sanitycheck/SoC_before_after.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4542f33c6d5db032",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T08:22:03.082750Z",
     "start_time": "2024-04-09T08:22:02.906034Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_base = d_in[d_in['outside_base']==True].copy()\n",
    "out_base['ch_flag'] = out_base['optimized_process_mean_power'].apply(lambda x: 1 if x>0 else 0)\n",
    "out_base['dis_flag'] = out_base['optimized_process_mean_power'].apply(lambda x: 1 if x<0 else 0)\n",
    "res = out_base.groupby('person').agg(ch_flag_sum = ('ch_flag','sum'),dis_flag_sum=('dis_flag','sum'))\n",
    "\n",
    "def label_bars(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 1.5),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "# Calculating frequency counts for each column\n",
    "ch = res['ch_flag_sum'].value_counts().sort_index()\n",
    "dis = res['dis_flag_sum'].value_counts().sort_index()\n",
    "\n",
    "# Ensuring both series have the same index to avoid plotting issues\n",
    "all_indices = ch.index.union(dis.index)\n",
    "ch = ch.reindex(all_indices, fill_value=0)\n",
    "dis = dis.reindex(all_indices, fill_value=0)\n",
    "\n",
    "# Plotting frequency counts side by side\n",
    "ind = np.arange(len(all_indices))  # the x locations for the groups\n",
    "width = 0.3  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bars_ch = ax.bar(ind - width/2, ch, width, label='charge_cnt', color='SkyBlue')\n",
    "bars_dis = ax.bar(ind + width/2, dis, width, label='discharge_cnt', color='IndianRed')\n",
    "\n",
    "label_bars(bars_ch)\n",
    "label_bars(bars_dis)\n",
    "# Adding some text for labels, title, and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Counts')\n",
    "ax.set_title('Distribution of Charge and Discharge times outside base profile per person')\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(all_indices)\n",
    "ax.legend()\n",
    "plt.savefig(f\"{path}/sanitycheck/ch_dis_times_outside_baseprofile.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5da1b4988e32fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T08:35:07.871046Z",
     "start_time": "2024-04-09T08:35:06.409803Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start charge hour \n",
    "ch = d_in[d_in['optimized_process_mean_power']>0].copy()\n",
    "ch['st_chg_hour'] = ch['shifted_st_chg_time'].apply(lambda x:x.strftime(\"%Y-%m-%d %H\"))\n",
    "ch['ed_chg_hour'] = ch['shifted_ed_chg_time'].apply(lambda x:x.strftime(\"%Y-%m-%d %H\"))\n",
    "ch['chg_durat'] = ch.apply(lambda row: (next(iter(row['process_list']))[1]-next(iter(row['process_list']))[0]).total_seconds()/3600, axis=1)\n",
    "\n",
    "dis = d_in[d_in['optimized_process_mean_power']<0].copy()\n",
    "dis['st_chg_hour'] = dis['shifted_st_chg_time'].apply(lambda x:x.strftime(\"%Y-%m-%d %H\"))\n",
    "dis['ed_chg_hour'] = dis['shifted_ed_chg_time'].apply(lambda x:x.strftime(\"%Y-%m-%d %H\"))\n",
    "dis['chg_durat'] = dis.apply(lambda row: (next(iter(row['process_list']))[1]-next(iter(row['process_list']))[0]).total_seconds()/3600, axis=1)\n",
    "\n",
    "charge_st_stat = ch.groupby('st_chg_hour').size().to_frame('cnt')\n",
    "charge_st_stat['prob'] = charge_st_stat['cnt']/charge_st_stat['cnt'].sum()\n",
    "charge_ed_stat = ch.groupby('ed_chg_hour').size().to_frame('cnt')\n",
    "charge_ed_stat['prob'] = charge_ed_stat['cnt']/charge_ed_stat['cnt'].sum()\n",
    "\n",
    "dis_st_stat = dis.groupby('st_chg_hour').size().to_frame('cnt')\n",
    "dis_st_stat['prob'] = dis_st_stat['cnt']/dis_st_stat['cnt'].sum()\n",
    "dis_ed_stat = dis.groupby('ed_chg_hour').size().to_frame('cnt')\n",
    "dis_ed_stat['prob'] = dis_ed_stat['cnt']/dis_ed_stat['cnt'].sum()\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10,20))\n",
    "axs[0].hist(ch['chg_durat'])\n",
    "axs[0].set_title('Charge Duration Probability Distribution')\n",
    "axs[1].bar(charge_st_stat.index.astype(str),charge_st_stat['prob'])\n",
    "axs[1].tick_params(axis='x',labelrotation=90)\n",
    "axs[1].set_title('Start Charging Time Probability Distribution')\n",
    "axs[2].bar(charge_ed_stat.index.astype(str),charge_ed_stat['prob'])\n",
    "axs[2].tick_params(axis='x',labelrotation=90)\n",
    "axs[2].set_title('End Charging Time Probability Distribution')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{path}/sanitycheck/charge_time_stats.png\")\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10,20))\n",
    "axs[0].hist(dis['chg_durat'])\n",
    "axs[0].set_title('Discharge Duration Probability Distribution')\n",
    "axs[1].bar(dis_st_stat.index.astype(str),dis_st_stat['prob'])\n",
    "axs[1].tick_params(axis='x',labelrotation=90)\n",
    "axs[1].set_title('Start Discharging Time Probability Distribution')\n",
    "axs[2].bar(dis_ed_stat.index.astype(str),dis_ed_stat['prob'])\n",
    "axs[2].tick_params(axis='x',labelrotation=90)\n",
    "axs[2].set_title('End Discharging Time Probability Distribution')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{path}/sanitycheck/discharge_time_stats.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88ef428",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(10, 6))\n",
    "flex_occur_place = flex_d_in.groupby('end_activity_type').size().to_frame('place_cnt')\n",
    "flex_occur_place_sorted = flex_occur_place.sort_values('place_cnt',ascending=True)\n",
    "axs.barh(flex_occur_place_sorted.index,flex_occur_place_sorted['place_cnt'])\n",
    "for index, value in enumerate(flex_occur_place_sorted['place_cnt']):\n",
    "    plt.text(value+20, index,str(value))\n",
    "plt.title('Flexibility occuring place type')\n",
    "plt.savefig(f\"{path}/sanitycheck/flex_occuring_place_type.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8bf687",
   "metadata": {},
   "outputs": [],
   "source": [
    "flex_ch_hour = (\n",
    "    d_in[d_in['optimized_process_mean_power']>0].groupby(\n",
    "        d_in['shifted_st_chg_time'].dt.strftime('%Y-%m-%d %H')\n",
    "    ).size().to_frame('hour_cnt')\n",
    ")\n",
    "flex_dis_hour = (\n",
    "    d_in[d_in['optimized_process_mean_power']<0].groupby(\n",
    "        d_in['shifted_st_chg_time'].dt.strftime('%Y-%m-%d %H')\n",
    "    ).size().to_frame('hour_cnt')\n",
    ")\n",
    "unshifted_hour = (\n",
    "    d_in[d_in['c']==True].groupby(\n",
    "        d_in['st_chg_time'].dt.strftime('%Y-%m-%d %H')\n",
    "    ).size().to_frame('hour_cnt')\n",
    ")\n",
    "\n",
    "# Merge the data frames on their indices (time), which are formed by the groupby operation\n",
    "combined = pd.merge(flex_ch_hour, flex_dis_hour, left_index=True, right_index=True, how='outer', suffixes=('_charge', '_discharge'))\n",
    "combined = pd.merge(combined, unshifted_hour, left_index=True, right_index=True, how='outer', suffixes=('', '_unshifted'))\n",
    "combined.fillna(0, inplace=True)  # Fill NaN with 0 for missing data\n",
    "combined.sort_index(inplace=True)  # Sort by index after merging\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.25\n",
    "\n",
    "# Get indices for bar positions\n",
    "indices = range(len(combined))\n",
    "\n",
    "# Plotting the bars\n",
    "axs.bar(indices, combined['hour_cnt_charge'], width=bar_width, alpha=0.7, color='blue', label='Shifted Charge Start Hour')\n",
    "axs.bar([i + bar_width for i in indices], combined['hour_cnt_discharge'], width=bar_width, alpha=0.7, color='red', label='Shifted Discharge Start Hour')\n",
    "axs.bar([i + 2 * bar_width for i in indices], combined['hour_cnt'], width=bar_width, alpha=0.7, color='green', label='Unshifted Start Hour')\n",
    "\n",
    "# Adding labels and title\n",
    "axs.set_title('Flexibility Occurrence by Hour: Charge vs. Discharge vs. Unshifted')\n",
    "axs.set_xlabel('Time (YYYY-MM-DD HH)')\n",
    "axs.set_ylabel('Count of Occurrences')\n",
    "\n",
    "# Set custom x-ticks to center them under the group of bars\n",
    "plt.xticks([i + bar_width for i in indices], combined.index, rotation=90)\n",
    "\n",
    "# Adding a legend\n",
    "axs.legend()\n",
    "\n",
    "# Adding grid lines\n",
    "axs.grid(True)\n",
    "\n",
    "\n",
    "plt.title('Compare Start Hour')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{path}/sanitycheck/start_hour.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e413b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad01cee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('vtog')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "607926bc357b26f0f53c8d80c5aea4598386ed08db661f7fe0f63ca501b6020f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
